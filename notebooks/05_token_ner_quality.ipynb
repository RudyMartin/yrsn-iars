{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Notebook 5: Token/NER Classification for Contract Entities\n",
    "\n",
    "**Objective**: Ensure accurate entity extraction from contracts for automated validation:\n",
    "- Detect token-level label issues (wrong entity type)\n",
    "- Identify entity boundary errors\n",
    "- Validate extracted amounts, dates, and parties\n",
    "\n",
    "---\n",
    "\n",
    "## Flow Diagram\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    subgraph Input[\"üìÑ Contract Document\"]\n",
    "        A[Contract Text]\n",
    "        B[Tokenized Sequence]\n",
    "    end\n",
    "\n",
    "    subgraph NER[\"üè∑Ô∏è Entity Extraction\"]\n",
    "        C[NER Model]\n",
    "        D[Token Predictions]\n",
    "        E[Entity Spans]\n",
    "        F[Prediction Probabilities]\n",
    "    end\n",
    "\n",
    "    subgraph Cleanlab[\"üßπ Cleanlab Token Classification\"]\n",
    "        G[find_label_issues - token]\n",
    "        H[get_label_quality_scores - token]\n",
    "        I[Per-Token Quality]\n",
    "        J[Entity-Level Quality]\n",
    "    end\n",
    "\n",
    "    subgraph Quality[\"üîç Entity Quality Analysis\"]\n",
    "        K{Token Issue Type?}\n",
    "        L[Wrong Entity Type ‚Üí N High]\n",
    "        M[Boundary Error ‚Üí S Moderate]\n",
    "        N[Low Confidence ‚Üí N/S]\n",
    "        O[Clean Extraction ‚Üí R High]\n",
    "    end\n",
    "\n",
    "    subgraph Critical[\"‚ö†Ô∏è Critical Entity Check\"]\n",
    "        P{Entity Type?}\n",
    "        Q[AMOUNT ‚Üí Verify value]\n",
    "        R[DATE ‚Üí Verify format]\n",
    "        S[PARTY ‚Üí Verify name]\n",
    "        T[POISONING if critical wrong]\n",
    "    end\n",
    "\n",
    "    subgraph Routing[\"üö¶ Temperature Routing\"]\n",
    "        U[Entity-Level œÑ = 1/R]\n",
    "        V{Trust Extraction?}\n",
    "        W[üü¢ GREEN: Use extracted values]\n",
    "        X[üü° YELLOW: Human verify]\n",
    "        Y[üî¥ RED: Manual extraction]\n",
    "    end\n",
    "\n",
    "    A --> B\n",
    "    B --> C\n",
    "    C --> D\n",
    "    C --> E\n",
    "    C --> F\n",
    "    D --> G\n",
    "    F --> G\n",
    "    F --> H\n",
    "    G --> I\n",
    "    H --> I\n",
    "    I --> J\n",
    "    J --> K\n",
    "    K --> L\n",
    "    K --> M\n",
    "    K --> N\n",
    "    K --> O\n",
    "    E --> P\n",
    "    P --> Q\n",
    "    P --> R\n",
    "    P --> S\n",
    "    L --> T\n",
    "    Q --> T\n",
    "    R --> T\n",
    "    S --> T\n",
    "    O --> U\n",
    "    M --> U\n",
    "    N --> U\n",
    "    T --> U\n",
    "    U --> V\n",
    "    V -->|High R| W\n",
    "    V -->|Medium R| X\n",
    "    V -->|Low R or Critical| Y\n",
    "\n",
    "    style NER fill:#e1f5fe\n",
    "    style Cleanlab fill:#fff3e0\n",
    "    style Critical fill:#ffebee\n",
    "    style Routing fill:#e8f5e9\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Collapse Type Focus**: POISONING (wrong amount/date extracted)\n",
    "\n",
    "**Difficulty**: ‚≠ê‚≠ê‚≠ê Hard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install cleanlab scikit-learn pandas numpy --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Cleanlab token classification\n",
    "from cleanlab.token_classification import (\n",
    "    filter as token_filter,\n",
    "    rank as token_rank\n",
    ")\n",
    "from cleanlab.token_classification.summary import display_issues\n",
    "\n",
    "# Import YRSN adapter\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from yrsn_iars.adapters.cleanlab_adapter import CleanlabAdapter, YRSNResult, CollapseType\n",
    "from yrsn_iars.adapters.temperature import compute_temperature\n",
    "\n",
    "print(\"Dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Define Entity Schema\n",
    "\n",
    "Contract entities we need to extract and validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity types for contract analysis\n",
    "ENTITY_TYPES = {\n",
    "    'O': 0,        # Outside any entity\n",
    "    'B-AMOUNT': 1, # Beginning of amount\n",
    "    'I-AMOUNT': 2, # Inside amount\n",
    "    'B-DATE': 3,   # Beginning of date\n",
    "    'I-DATE': 4,   # Inside date\n",
    "    'B-PARTY': 5,  # Beginning of party name\n",
    "    'I-PARTY': 6,  # Inside party name\n",
    "    'B-TERM': 7,   # Beginning of term/duration\n",
    "    'I-TERM': 8,   # Inside term\n",
    "}\n",
    "\n",
    "ID_TO_ENTITY = {v: k for k, v in ENTITY_TYPES.items()}\n",
    "NUM_CLASSES = len(ENTITY_TYPES)\n",
    "\n",
    "# Critical entities (errors here are POISONING)\n",
    "CRITICAL_ENTITIES = ['B-AMOUNT', 'I-AMOUNT', 'B-DATE', 'I-DATE']\n",
    "\n",
    "print(f\"Entity types: {list(ENTITY_TYPES.keys())}\")\n",
    "print(f\"Critical entities: {CRITICAL_ENTITIES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Generate Synthetic Contract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "def generate_contract_sentence():\n",
    "    \"\"\"Generate a synthetic contract sentence with entities.\"\"\"\n",
    "    templates = [\n",
    "        (\"Agreement between {party1} and {party2} dated {date} for {amount} .\",\n",
    "         [(2, 'PARTY'), (4, 'PARTY'), (6, 'DATE'), (8, 'AMOUNT')]),\n",
    "        (\"{party1} agrees to pay {amount} to {party2} by {date} .\",\n",
    "         [(0, 'PARTY'), (4, 'AMOUNT'), (6, 'PARTY'), (8, 'DATE')]),\n",
    "        (\"Contract term : {term} starting {date} with value {amount} .\",\n",
    "         [(3, 'TERM'), (5, 'DATE'), (8, 'AMOUNT')]),\n",
    "        (\"Payment of {amount} due on {date} from {party1} .\",\n",
    "         [(2, 'AMOUNT'), (5, 'DATE'), (7, 'PARTY')]),\n",
    "    ]\n",
    "    \n",
    "    parties = [\"Acme Corp\", \"Tech Inc\", \"Global LLC\", \"Alpha Partners\", \"Beta Services\"]\n",
    "    amounts = [\"$50,000\", \"$100,000\", \"$250,000\", \"$1,000,000\", \"$75,500\"]\n",
    "    dates = [\"January 15, 2024\", \"March 1, 2024\", \"December 31, 2023\", \"July 20, 2024\"]\n",
    "    terms = [\"12 months\", \"24 months\", \"36 months\", \"6 months\"]\n",
    "    \n",
    "    template, entity_positions = templates[np.random.randint(len(templates))]\n",
    "    \n",
    "    # Fill template\n",
    "    text = template.format(\n",
    "        party1=np.random.choice(parties),\n",
    "        party2=np.random.choice(parties),\n",
    "        amount=np.random.choice(amounts),\n",
    "        date=np.random.choice(dates),\n",
    "        term=np.random.choice(terms)\n",
    "    )\n",
    "    \n",
    "    return text, entity_positions\n",
    "\n",
    "# Generate synthetic dataset\n",
    "n_sentences = 100\n",
    "sentences_data = []\n",
    "\n",
    "for i in range(n_sentences):\n",
    "    text, entity_pos = generate_contract_sentence()\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Create labels\n",
    "    labels = [0] * len(tokens)  # Default: O\n",
    "    for pos, entity_type in entity_pos:\n",
    "        if pos < len(tokens):\n",
    "            # Handle multi-token entities\n",
    "            labels[pos] = ENTITY_TYPES[f'B-{entity_type}']\n",
    "            # Check if next token is also part of entity (simplified)\n",
    "            if pos + 1 < len(tokens) and tokens[pos + 1] not in ['.', ',', 'and', 'to', 'by', 'from', 'for', 'with']:\n",
    "                if np.random.random() > 0.5:  # 50% chance of multi-token\n",
    "                    labels[pos + 1] = ENTITY_TYPES[f'I-{entity_type}']\n",
    "    \n",
    "    sentences_data.append({\n",
    "        'sentence_id': i,\n",
    "        'text': text,\n",
    "        'tokens': tokens,\n",
    "        'labels': labels,\n",
    "        'n_tokens': len(tokens)\n",
    "    })\n",
    "\n",
    "sentences_df = pd.DataFrame(sentences_data)\n",
    "print(f\"Generated {len(sentences_df)} contract sentences\")\n",
    "print(f\"\\nSample:\")\n",
    "print(f\"Text: {sentences_df.iloc[0]['text']}\")\n",
    "print(f\"Tokens: {sentences_df.iloc[0]['tokens']}\")\n",
    "print(f\"Labels: {[ID_TO_ENTITY[l] for l in sentences_df.iloc[0]['labels']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Simulate NER Predictions with Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_ner_predictions(tokens: List[str], true_labels: List[int], \n",
    "                              error_rate: float = 0.15) -> Tuple[List[int], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Simulate NER model predictions with controlled error rate.\n",
    "    Returns predicted labels and prediction probabilities.\n",
    "    \"\"\"\n",
    "    n_tokens = len(tokens)\n",
    "    pred_labels = true_labels.copy()\n",
    "    pred_probs = np.zeros((n_tokens, NUM_CLASSES))\n",
    "    \n",
    "    for i in range(n_tokens):\n",
    "        true_label = true_labels[i]\n",
    "        \n",
    "        if np.random.random() < error_rate:\n",
    "            # Introduce error\n",
    "            error_type = np.random.choice(['wrong_type', 'boundary', 'miss'])\n",
    "            \n",
    "            if error_type == 'wrong_type' and true_label > 0:\n",
    "                # Predict wrong entity type\n",
    "                wrong_labels = [l for l in range(NUM_CLASSES) if l != true_label and l != 0]\n",
    "                pred_labels[i] = np.random.choice(wrong_labels)\n",
    "                # Lower confidence\n",
    "                pred_probs[i, pred_labels[i]] = 0.4 + np.random.random() * 0.3\n",
    "                pred_probs[i, true_label] = 0.2 + np.random.random() * 0.2\n",
    "                \n",
    "            elif error_type == 'boundary':\n",
    "                # Boundary error: B/I confusion\n",
    "                if ID_TO_ENTITY[true_label].startswith('B-'):\n",
    "                    pred_labels[i] = ENTITY_TYPES[ID_TO_ENTITY[true_label].replace('B-', 'I-')]\n",
    "                elif ID_TO_ENTITY[true_label].startswith('I-'):\n",
    "                    pred_labels[i] = ENTITY_TYPES[ID_TO_ENTITY[true_label].replace('I-', 'B-')]\n",
    "                pred_probs[i, pred_labels[i]] = 0.5 + np.random.random() * 0.3\n",
    "                pred_probs[i, true_label] = 0.3 + np.random.random() * 0.2\n",
    "                \n",
    "            else:  # miss\n",
    "                # Miss entity entirely (predict O)\n",
    "                pred_labels[i] = 0\n",
    "                pred_probs[i, 0] = 0.5 + np.random.random() * 0.3\n",
    "                pred_probs[i, true_label] = 0.2 + np.random.random() * 0.2\n",
    "        else:\n",
    "            # Correct prediction\n",
    "            pred_labels[i] = true_label\n",
    "            pred_probs[i, true_label] = 0.85 + np.random.random() * 0.14\n",
    "        \n",
    "        # Distribute remaining probability\n",
    "        remaining = 1.0 - pred_probs[i].sum()\n",
    "        noise = np.random.random(NUM_CLASSES)\n",
    "        noise[pred_probs[i] > 0] = 0\n",
    "        if noise.sum() > 0:\n",
    "            pred_probs[i] += remaining * (noise / noise.sum())\n",
    "    \n",
    "    return pred_labels, pred_probs\n",
    "\n",
    "# Generate predictions for all sentences\n",
    "all_tokens = []\n",
    "all_true_labels = []\n",
    "all_pred_labels = []\n",
    "all_pred_probs = []\n",
    "sentence_boundaries = [0]\n",
    "\n",
    "for _, row in sentences_df.iterrows():\n",
    "    pred_labels, pred_probs = simulate_ner_predictions(\n",
    "        row['tokens'], row['labels'], error_rate=0.12\n",
    "    )\n",
    "    \n",
    "    all_tokens.extend(row['tokens'])\n",
    "    all_true_labels.extend(row['labels'])\n",
    "    all_pred_labels.extend(pred_labels)\n",
    "    all_pred_probs.append(pred_probs)\n",
    "    sentence_boundaries.append(sentence_boundaries[-1] + len(row['tokens']))\n",
    "\n",
    "all_pred_probs = np.vstack(all_pred_probs)\n",
    "all_true_labels = np.array(all_true_labels)\n",
    "all_pred_labels = np.array(all_pred_labels)\n",
    "\n",
    "print(f\"Total tokens: {len(all_tokens)}\")\n",
    "print(f\"Prediction accuracy: {(all_pred_labels == all_true_labels).mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 5. Cleanlab Token Classification Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to format expected by Cleanlab token classification\n",
    "# Cleanlab expects list of sentences, each with list of token labels/probs\n",
    "\n",
    "labels_per_sentence = []\n",
    "pred_probs_per_sentence = []\n",
    "\n",
    "for i in range(len(sentences_df)):\n",
    "    start = sentence_boundaries[i]\n",
    "    end = sentence_boundaries[i + 1]\n",
    "    labels_per_sentence.append(all_true_labels[start:end].tolist())\n",
    "    pred_probs_per_sentence.append(all_pred_probs[start:end])\n",
    "\n",
    "print(f\"Prepared {len(labels_per_sentence)} sentences for Cleanlab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find token-level label issues\n",
    "token_issues = token_filter.find_label_issues(\n",
    "    labels=labels_per_sentence,\n",
    "    pred_probs=pred_probs_per_sentence\n",
    ")\n",
    "\n",
    "# Get quality scores\n",
    "token_quality = token_rank.get_label_quality_scores(\n",
    "    labels=labels_per_sentence,\n",
    "    pred_probs=pred_probs_per_sentence\n",
    ")\n",
    "\n",
    "print(f\"Found token issues in {sum(1 for issues in token_issues if any(issues))} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add results to sentences dataframe\n",
    "sentences_df['token_issues'] = token_issues\n",
    "sentences_df['token_quality'] = token_quality\n",
    "sentences_df['n_issues'] = [sum(issues) for issues in token_issues]\n",
    "sentences_df['min_quality'] = [min(q) if len(q) > 0 else 1.0 for q in token_quality]\n",
    "sentences_df['mean_quality'] = [np.mean(q) if len(q) > 0 else 1.0 for q in token_quality]\n",
    "\n",
    "print(\"Token Quality Statistics:\")\n",
    "print(sentences_df[['n_issues', 'min_quality', 'mean_quality']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 6. Aggregate to Entity-Level YRSN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_with_quality(tokens, labels, quality_scores, issues):\n",
    "    \"\"\"Extract entities and compute per-entity quality.\"\"\"\n",
    "    entities = []\n",
    "    current_entity = None\n",
    "    \n",
    "    for i, (token, label, quality, is_issue) in enumerate(zip(tokens, labels, quality_scores, issues)):\n",
    "        entity_name = ID_TO_ENTITY[label]\n",
    "        \n",
    "        if entity_name.startswith('B-'):\n",
    "            # Save previous entity\n",
    "            if current_entity:\n",
    "                entities.append(current_entity)\n",
    "            \n",
    "            # Start new entity\n",
    "            entity_type = entity_name[2:]\n",
    "            current_entity = {\n",
    "                'type': entity_type,\n",
    "                'tokens': [token],\n",
    "                'start': i,\n",
    "                'end': i,\n",
    "                'qualities': [quality],\n",
    "                'has_issue': [is_issue],\n",
    "                'is_critical': entity_name in CRITICAL_ENTITIES\n",
    "            }\n",
    "            \n",
    "        elif entity_name.startswith('I-') and current_entity:\n",
    "            # Continue entity\n",
    "            current_entity['tokens'].append(token)\n",
    "            current_entity['end'] = i\n",
    "            current_entity['qualities'].append(quality)\n",
    "            current_entity['has_issue'].append(is_issue)\n",
    "            \n",
    "        else:\n",
    "            # O tag - save current entity\n",
    "            if current_entity:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = None\n",
    "    \n",
    "    # Don't forget last entity\n",
    "    if current_entity:\n",
    "        entities.append(current_entity)\n",
    "    \n",
    "    # Compute entity-level quality\n",
    "    for entity in entities:\n",
    "        entity['text'] = ' '.join(entity['tokens'])\n",
    "        entity['min_quality'] = min(entity['qualities'])\n",
    "        entity['mean_quality'] = np.mean(entity['qualities'])\n",
    "        entity['any_issue'] = any(entity['has_issue'])\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Extract entities for all sentences\n",
    "all_entities = []\n",
    "for _, row in sentences_df.iterrows():\n",
    "    entities = extract_entities_with_quality(\n",
    "        row['tokens'], row['labels'], \n",
    "        row['token_quality'], row['token_issues']\n",
    "    )\n",
    "    for entity in entities:\n",
    "        entity['sentence_id'] = row['sentence_id']\n",
    "        all_entities.append(entity)\n",
    "\n",
    "entities_df = pd.DataFrame(all_entities)\n",
    "print(f\"Extracted {len(entities_df)} entities\")\n",
    "print(f\"\\nEntity type distribution:\")\n",
    "print(entities_df['type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 7. YRSN Decomposition for Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter = CleanlabAdapter()\n",
    "\n",
    "def compute_entity_yrsn(row):\n",
    "    \"\"\"Compute YRSN for a single entity extraction.\"\"\"\n",
    "    \n",
    "    # N (Noise): Issues in extraction\n",
    "    n_from_quality = 1 - row['min_quality']\n",
    "    n_from_issues = 0.3 if row['any_issue'] else 0.0\n",
    "    \n",
    "    # Critical entities get higher N if issues exist\n",
    "    if row['is_critical'] and row['any_issue']:\n",
    "        n_from_issues += 0.3  # POISONING risk\n",
    "    \n",
    "    N = min(1.0, n_from_quality * 0.6 + n_from_issues)\n",
    "    \n",
    "    # S (Superfluous): Boundary uncertainty\n",
    "    # Multi-token entities with quality variance ‚Üí boundary issues\n",
    "    quality_variance = np.var(row['qualities']) if len(row['qualities']) > 1 else 0\n",
    "    S = min(0.5, quality_variance * 2)\n",
    "    S = min(1.0 - N, S)\n",
    "    \n",
    "    # R (Relevant): Clean extraction\n",
    "    R = max(0, 1.0 - N - S)\n",
    "    \n",
    "    # Normalize\n",
    "    total = R + S + N\n",
    "    return YRSNResult(R=R/total, S=S/total, N=N/total)\n",
    "\n",
    "# Apply YRSN computation\n",
    "yrsn_results = entities_df.apply(compute_entity_yrsn, axis=1)\n",
    "entities_df['R'] = [y.R for y in yrsn_results]\n",
    "entities_df['S'] = [y.S for y in yrsn_results]\n",
    "entities_df['N'] = [y.N for y in yrsn_results]\n",
    "entities_df['collapse_type'] = [y.collapse_type.value for y in yrsn_results]\n",
    "\n",
    "# Mark POISONING for critical entities with issues\n",
    "entities_df.loc[\n",
    "    (entities_df['is_critical']) & (entities_df['N'] > 0.3),\n",
    "    'collapse_type'\n",
    "] = 'poisoning'\n",
    "\n",
    "print(\"Entity YRSN Statistics:\")\n",
    "print(entities_df[['R', 'S', 'N']].describe())\n",
    "print(f\"\\nCollapse distribution:\")\n",
    "print(entities_df['collapse_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 8. Temperature-Based Routing for Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute temperature per entity\n",
    "entities_df['temperature'] = entities_df['R'].apply(lambda r: compute_temperature(r))\n",
    "\n",
    "# NER-specific routing (tighter thresholds for critical entities)\n",
    "def route_entity(row):\n",
    "    tau = row['temperature']\n",
    "    is_critical = row['is_critical']\n",
    "    collapse = row['collapse_type']\n",
    "    \n",
    "    # POISONING on critical entity: always red\n",
    "    if collapse == 'poisoning' and is_critical:\n",
    "        return 'red'\n",
    "    \n",
    "    # Critical entities use tighter thresholds\n",
    "    if is_critical:\n",
    "        if tau < 1.2 and row['R'] > 0.7:\n",
    "            return 'green'\n",
    "        elif tau < 1.5:\n",
    "            return 'yellow'\n",
    "        else:\n",
    "            return 'red'\n",
    "    else:\n",
    "        # Non-critical entities\n",
    "        if tau < 1.5:\n",
    "            return 'green'\n",
    "        elif tau < 2.5:\n",
    "            return 'yellow'\n",
    "        else:\n",
    "            return 'red'\n",
    "\n",
    "entities_df['stream'] = entities_df.apply(route_entity, axis=1)\n",
    "\n",
    "print(\"Entity Routing Distribution:\")\n",
    "print(entities_df['stream'].value_counts())\n",
    "\n",
    "print(\"\\nRouting by Entity Type:\")\n",
    "print(pd.crosstab(entities_df['type'], entities_df['stream'], normalize='index').round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 9. Identify POISONING Cases (Critical Extraction Errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find POISONING cases (critical entity extraction errors)\n",
    "poisoning_cases = entities_df[\n",
    "    (entities_df['collapse_type'] == 'poisoning') | \n",
    "    ((entities_df['is_critical']) & (entities_df['N'] > 0.25))\n",
    "]\n",
    "\n",
    "print(\"POISONING Cases (Critical Entity Extraction Errors):\")\n",
    "print(\"=\"*60)\n",
    "for _, row in poisoning_cases.head(10).iterrows():\n",
    "    print(f\"\\n[Sentence {row['sentence_id']}] Entity: '{row['text']}'\")\n",
    "    print(f\"  Type: {row['type']} (CRITICAL)\")\n",
    "    print(f\"  YRSN: R={row['R']:.2f}, S={row['S']:.2f}, N={row['N']:.2f}\")\n",
    "    print(f\"  Min Quality: {row['min_quality']:.3f}\")\n",
    "    print(f\"  Has Issue: {row['any_issue']}\")\n",
    "    print(f\"  Stream: {row['stream'].upper()}, œÑ={row['temperature']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 10. Aggregate to Sentence-Level Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each sentence, determine overall routing based on worst entity\n",
    "def aggregate_sentence_routing(sentence_entities):\n",
    "    \"\"\"Aggregate entity-level routing to sentence-level.\"\"\"\n",
    "    if len(sentence_entities) == 0:\n",
    "        return {'stream': 'green', 'worst_entity': None, 'n_entities': 0}\n",
    "    \n",
    "    # Sentence stream is worst of all entities\n",
    "    stream_priority = {'red': 0, 'yellow': 1, 'green': 2}\n",
    "    worst_idx = sentence_entities['stream'].map(stream_priority).idxmin()\n",
    "    worst_row = sentence_entities.loc[worst_idx]\n",
    "    \n",
    "    return {\n",
    "        'stream': worst_row['stream'],\n",
    "        'worst_entity_type': worst_row['type'],\n",
    "        'worst_entity_text': worst_row['text'],\n",
    "        'worst_R': worst_row['R'],\n",
    "        'n_entities': len(sentence_entities),\n",
    "        'n_critical': sentence_entities['is_critical'].sum(),\n",
    "        'has_poisoning': (sentence_entities['collapse_type'] == 'poisoning').any()\n",
    "    }\n",
    "\n",
    "# Aggregate\n",
    "sentence_routing = entities_df.groupby('sentence_id').apply(\n",
    "    aggregate_sentence_routing\n",
    ").apply(pd.Series)\n",
    "\n",
    "sentences_df = sentences_df.merge(sentence_routing, left_on='sentence_id', right_index=True)\n",
    "\n",
    "print(\"Sentence-Level Routing:\")\n",
    "print(sentences_df['stream'].value_counts())\n",
    "print(f\"\\nSentences with POISONING risk: {sentences_df['has_poisoning'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 11. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save entity-level results\n",
    "entity_cols = ['sentence_id', 'type', 'text', 'is_critical', \n",
    "               'min_quality', 'R', 'S', 'N', 'collapse_type', 'temperature', 'stream']\n",
    "entities_df[entity_cols].to_csv('entity_yrsn_results.csv', index=False)\n",
    "\n",
    "# Save sentence-level results\n",
    "sentence_cols = ['sentence_id', 'text', 'n_issues', 'mean_quality', \n",
    "                 'stream', 'worst_entity_type', 'has_poisoning', 'n_critical']\n",
    "sentences_df[sentence_cols].to_csv('sentence_yrsn_results.csv', index=False)\n",
    "\n",
    "print(f\"Saved {len(entities_df)} entities and {len(sentences_df)} sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we:\n",
    "1. Generated synthetic contract text with entity annotations\n",
    "2. Simulated NER model predictions with controlled error rates\n",
    "3. Used Cleanlab token classification to find label issues\n",
    "4. Aggregated token quality to entity-level YRSN\n",
    "5. Applied stricter routing for critical entities (AMOUNT, DATE)\n",
    "6. Identified POISONING collapse cases (critical extraction errors)\n",
    "7. Aggregated to sentence-level routing decisions\n",
    "\n",
    "**Key Insight**: For NER in contracts, AMOUNT and DATE entities are critical. Any extraction error on these is POISONING and forces RED routing for manual verification. Temperature is kept tight (œÑ < 1.2) for critical entities.\n",
    "\n",
    "**Next**: Notebook 6 - Production Pipeline (Full IARS Integration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
