{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Notebook 4: RAG/Retrieval Context Quality for Policy Documents\n",
    "\n",
    "**Objective**: Ensure approval decisions use relevant, current policy documents:\n",
    "- Detect out-of-distribution queries (novel request types)\n",
    "- Evaluate retrieval context quality\n",
    "- Handle irrelevant or outdated policy matches\n",
    "\n",
    "---\n",
    "\n",
    "## Flow Diagram\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    subgraph Input[\"ðŸ“¥ Approval Request\"]\n",
    "        A[Request Text]\n",
    "        B[Request Embedding]\n",
    "    end\n",
    "\n",
    "    subgraph RAG[\"ðŸ“š Policy Retrieval\"]\n",
    "        C[Policy Document Store]\n",
    "        D[Semantic Search]\n",
    "        E[Top-K Retrieved Chunks]\n",
    "        F[Retrieval Scores]\n",
    "    end\n",
    "\n",
    "    subgraph Cleanlab[\"ðŸ§¹ Cleanlab OOD Analysis\"]\n",
    "        G[OutOfDistribution.fit]\n",
    "        H[OOD Score per Query]\n",
    "        I[find_overlapping_classes]\n",
    "        J[Policy Category Confusion]\n",
    "    end\n",
    "\n",
    "    subgraph Quality[\"ðŸ” Context Quality\"]\n",
    "        K{Query In-Distribution?}\n",
    "        L[High OOD â†’ Novel Query â†’ N High]\n",
    "        M[Low Retrieval Score â†’ Irrelevant â†’ S High]\n",
    "        N[Category Confusion â†’ N Moderate]\n",
    "        O[Good Match â†’ R High]\n",
    "    end\n",
    "\n",
    "    subgraph YRSN[\"ðŸŽ¯ YRSN Decomposition\"]\n",
    "        P[Combine Signals]\n",
    "        Q[CONFUSION Collapse Check]\n",
    "    end\n",
    "\n",
    "    subgraph Routing[\"ðŸš¦ Temperature Routing\"]\n",
    "        R[Compute Ï„ = 1/Î±]\n",
    "        S{Context Quality?}\n",
    "        T[ðŸŸ¢ GREEN: Trust context]\n",
    "        U[ðŸŸ¡ YELLOW: Human verify context]\n",
    "        V[ðŸ”´ RED: Manual policy lookup]\n",
    "    end\n",
    "\n",
    "    A --> B\n",
    "    B --> D\n",
    "    C --> D\n",
    "    D --> E\n",
    "    D --> F\n",
    "    B --> G\n",
    "    G --> H\n",
    "    E --> I\n",
    "    I --> J\n",
    "    H --> K\n",
    "    K -->|OOD| L\n",
    "    F --> K\n",
    "    K -->|Low score| M\n",
    "    J --> N\n",
    "    K -->|Good| O\n",
    "    L --> P\n",
    "    M --> P\n",
    "    N --> P\n",
    "    O --> P\n",
    "    P --> Q\n",
    "    Q --> R\n",
    "    R --> S\n",
    "    S -->|High R| T\n",
    "    S -->|Medium R| U\n",
    "    S -->|Low R| V\n",
    "\n",
    "    style RAG fill:#e1f5fe\n",
    "    style Cleanlab fill:#fff3e0\n",
    "    style Quality fill:#e8f5e9\n",
    "    style YRSN fill:#fce4ec\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Collapse Type Focus**: CONFUSION (wrong policy retrieved)\n",
    "\n",
    "**Difficulty**: â­â­ Medium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install cleanlab sentence-transformers scikit-learn pandas faiss-cpu --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# Cleanlab for OOD detection\n",
    "from cleanlab.outlier import OutOfDistribution\n",
    "\n",
    "# Import YRSN adapter\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from yrsn_iars.adapters.cleanlab_adapter import CleanlabAdapter, YRSNResult, CollapseType\n",
    "from yrsn_iars.adapters.temperature import compute_temperature\n",
    "\n",
    "print(\"Dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Create Policy Document Store\n",
    "\n",
    "Simulate a knowledge base of corporate policy documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy document corpus\n",
    "policy_documents = [\n",
    "    # Travel Policy\n",
    "    {\"category\": \"travel\", \"doc\": \"travel_policy_v2.pdf\", \n",
    "     \"text\": \"Travel expenses must be pre-approved for amounts exceeding $500. Business class flights require VP approval for trips over 6 hours. Hotel expenses capped at $250/night.\"},\n",
    "    {\"category\": \"travel\", \"doc\": \"travel_policy_v2.pdf\",\n",
    "     \"text\": \"International travel requires 2-week advance notice and security review. Per diem rates follow GSA guidelines.\"},\n",
    "    \n",
    "    # Expense Policy\n",
    "    {\"category\": \"expense\", \"doc\": \"expense_policy_v3.pdf\",\n",
    "     \"text\": \"Team meals and entertainment expenses require itemized receipts. Maximum $100 per person for client dinners.\"},\n",
    "    {\"category\": \"expense\", \"doc\": \"expense_policy_v3.pdf\",\n",
    "     \"text\": \"Home office equipment up to $500 can be expensed once per year. Must be job-related.\"},\n",
    "    \n",
    "    # Software/License Policy\n",
    "    {\"category\": \"software\", \"doc\": \"software_procurement.pdf\",\n",
    "     \"text\": \"Software licenses must be procured through IT. Cloud services require security review. Annual license renewals auto-approved if under $10,000.\"},\n",
    "    {\"category\": \"software\", \"doc\": \"software_procurement.pdf\",\n",
    "     \"text\": \"Open source software usage requires legal review. No unapproved SaaS tools for customer data.\"},\n",
    "    \n",
    "    # Vendor Policy\n",
    "    {\"category\": \"vendor\", \"doc\": \"vendor_management.pdf\",\n",
    "     \"text\": \"New vendors require due diligence review. Contracts over $50,000 need legal review. Preferred vendor list maintained by procurement.\"},\n",
    "    {\"category\": \"vendor\", \"doc\": \"vendor_management.pdf\",\n",
    "     \"text\": \"Vendor payments net-30 unless negotiated. Early payment discounts require CFO approval.\"},\n",
    "    \n",
    "    # Equipment Policy\n",
    "    {\"category\": \"equipment\", \"doc\": \"asset_management.pdf\",\n",
    "     \"text\": \"IT equipment refresh cycle is 3 years for laptops. Requests for non-standard equipment require manager justification.\"},\n",
    "    {\"category\": \"equipment\", \"doc\": \"asset_management.pdf\",\n",
    "     \"text\": \"Equipment disposal must follow data destruction protocols. Returns processed within 30 days of employee departure.\"},\n",
    "]\n",
    "\n",
    "policies_df = pd.DataFrame(policy_documents)\n",
    "print(f\"Policy corpus: {len(policies_df)} chunks from {policies_df['doc'].nunique()} documents\")\n",
    "print(f\"Categories: {policies_df['category'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings for policy documents\n",
    "encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(\"Embedding policy documents...\")\n",
    "policy_embeddings = encoder.encode(policies_df['text'].tolist(), show_progress_bar=True)\n",
    "\n",
    "print(f\"Policy embeddings shape: {policy_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build FAISS index for retrieval\n",
    "dimension = policy_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner product (cosine sim for normalized vectors)\n",
    "\n",
    "# Normalize for cosine similarity\n",
    "faiss.normalize_L2(policy_embeddings)\n",
    "index.add(policy_embeddings)\n",
    "\n",
    "print(f\"FAISS index built with {index.ntotal} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 3. Generate Approval Requests (Including OOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mix of in-distribution and OOD requests\n",
    "requests = [\n",
    "    # In-distribution (should match policies)\n",
    "    {\"id\": \"REQ-001\", \"text\": \"Can I expense a team lunch for 5 people at $80 per person?\", \"expected_category\": \"expense\"},\n",
    "    {\"id\": \"REQ-002\", \"text\": \"Need approval for AWS license renewal, annual cost $8,000\", \"expected_category\": \"software\"},\n",
    "    {\"id\": \"REQ-003\", \"text\": \"Requesting business class flight to London (8 hour flight)\", \"expected_category\": \"travel\"},\n",
    "    {\"id\": \"REQ-004\", \"text\": \"New vendor onboarding for consulting services, $75,000 contract\", \"expected_category\": \"vendor\"},\n",
    "    {\"id\": \"REQ-005\", \"text\": \"Request new laptop replacement, current one is 4 years old\", \"expected_category\": \"equipment\"},\n",
    "    {\"id\": \"REQ-006\", \"text\": \"Per diem for 5-day conference in Chicago\", \"expected_category\": \"travel\"},\n",
    "    {\"id\": \"REQ-007\", \"text\": \"Home office monitor and keyboard expense, $450 total\", \"expected_category\": \"expense\"},\n",
    "    {\"id\": \"REQ-008\", \"text\": \"Salesforce license for new sales team member\", \"expected_category\": \"software\"},\n",
    "    \n",
    "    # Out-of-distribution (novel request types)\n",
    "    {\"id\": \"REQ-009\", \"text\": \"Request for cryptocurrency mining hardware for R&D project\", \"expected_category\": \"OOD\"},\n",
    "    {\"id\": \"REQ-010\", \"text\": \"Need approval for employee wellness retreat in Bali\", \"expected_category\": \"OOD\"},\n",
    "    {\"id\": \"REQ-011\", \"text\": \"Requesting company sponsorship for my kid's little league team\", \"expected_category\": \"OOD\"},\n",
    "    {\"id\": \"REQ-012\", \"text\": \"Can we hire a DJ for the office holiday party?\", \"expected_category\": \"OOD\"},\n",
    "    \n",
    "    # Ambiguous (could match multiple policies)\n",
    "    {\"id\": \"REQ-013\", \"text\": \"Need both software and travel for remote conference with cloud vendor\", \"expected_category\": \"ambiguous\"},\n",
    "    {\"id\": \"REQ-014\", \"text\": \"Equipment purchase from new vendor, need expedited payment\", \"expected_category\": \"ambiguous\"},\n",
    "]\n",
    "\n",
    "requests_df = pd.DataFrame(requests)\n",
    "print(f\"Generated {len(requests_df)} test requests\")\n",
    "print(f\"\\nCategory distribution:\")\n",
    "print(requests_df['expected_category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. Perform Retrieval and OOD Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed requests\n",
    "request_embeddings = encoder.encode(requests_df['text'].tolist())\n",
    "faiss.normalize_L2(request_embeddings)\n",
    "\n",
    "# Retrieve top-3 policy chunks for each request\n",
    "k = 3\n",
    "distances, indices = index.search(request_embeddings, k)\n",
    "\n",
    "# Store retrieval results\n",
    "retrieval_results = []\n",
    "for i, (req_id, dists, idxs) in enumerate(zip(requests_df['id'], distances, indices)):\n",
    "    for rank, (dist, idx) in enumerate(zip(dists, idxs)):\n",
    "        retrieval_results.append({\n",
    "            'request_id': req_id,\n",
    "            'rank': rank + 1,\n",
    "            'policy_idx': idx,\n",
    "            'policy_category': policies_df.iloc[idx]['category'],\n",
    "            'policy_doc': policies_df.iloc[idx]['doc'],\n",
    "            'similarity': dist\n",
    "        })\n",
    "\n",
    "retrieval_df = pd.DataFrame(retrieval_results)\n",
    "print(\"Top retrieval results:\")\n",
    "retrieval_df.head(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanlab OOD Detection\n",
    "# Train OOD detector on policy embeddings\n",
    "ood = OutOfDistribution()\n",
    "ood.fit(policy_embeddings)\n",
    "\n",
    "# Score each request\n",
    "ood_scores = ood.score(request_embeddings)\n",
    "\n",
    "# Note: Cleanlab OOD score: higher = more likely OOD\n",
    "# We invert for YRSN: higher = more in-distribution\n",
    "requests_df['ood_score_raw'] = ood_scores\n",
    "requests_df['in_distribution_score'] = 1 - ood_scores\n",
    "\n",
    "print(\"OOD Scores:\")\n",
    "print(requests_df[['id', 'expected_category', 'ood_score_raw', 'in_distribution_score']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 5. Compute Context Quality Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate retrieval quality per request\n",
    "def compute_retrieval_quality(request_id):\n",
    "    \"\"\"Compute retrieval quality metrics for a request.\"\"\"\n",
    "    req_results = retrieval_df[retrieval_df['request_id'] == request_id]\n",
    "    \n",
    "    # Top-1 similarity\n",
    "    top1_sim = req_results.iloc[0]['similarity']\n",
    "    \n",
    "    # Mean similarity of top-3\n",
    "    mean_sim = req_results['similarity'].mean()\n",
    "    \n",
    "    # Category consistency (do retrieved chunks agree?)\n",
    "    categories = req_results['policy_category'].tolist()\n",
    "    category_agreement = len(set(categories)) == 1\n",
    "    \n",
    "    # Similarity gap (top-1 vs top-2)\n",
    "    sim_gap = req_results.iloc[0]['similarity'] - req_results.iloc[1]['similarity']\n",
    "    \n",
    "    return {\n",
    "        'top1_similarity': top1_sim,\n",
    "        'mean_similarity': mean_sim,\n",
    "        'category_agreement': category_agreement,\n",
    "        'similarity_gap': sim_gap,\n",
    "        'top_category': categories[0]\n",
    "    }\n",
    "\n",
    "# Apply to all requests\n",
    "quality_metrics = requests_df['id'].apply(compute_retrieval_quality).apply(pd.Series)\n",
    "requests_df = pd.concat([requests_df, quality_metrics], axis=1)\n",
    "\n",
    "print(\"Retrieval Quality Metrics:\")\n",
    "requests_df[['id', 'expected_category', 'top1_similarity', 'category_agreement', 'top_category']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 6. YRSN Decomposition for RAG Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter = CleanlabAdapter()\n",
    "\n",
    "def compute_rag_yrsn(row):\n",
    "    \"\"\"Compute YRSN specifically for RAG context quality.\"\"\"\n",
    "    \n",
    "    # N (Noise): OOD queries and category confusion\n",
    "    n_ood = row['ood_score_raw']  # Higher = more OOD = more N\n",
    "    n_confusion = 0.2 if not row['category_agreement'] else 0.0\n",
    "    N = min(1.0, n_ood + n_confusion)\n",
    "    \n",
    "    # S (Superfluous): Low similarity (irrelevant context)\n",
    "    # If similarity is low, context is not useful\n",
    "    s_irrelevant = max(0, 0.7 - row['top1_similarity'])  # Below 0.7 = irrelevant\n",
    "    s_ambiguous = max(0, 0.15 - row['similarity_gap'])   # Small gap = ambiguous\n",
    "    S = min(1.0 - N, s_irrelevant + s_ambiguous)\n",
    "    \n",
    "    # R (Relevant): Good context quality\n",
    "    R = max(0, 1.0 - N - S)\n",
    "    \n",
    "    # Normalize\n",
    "    total = R + S + N\n",
    "    return YRSNResult(R=R/total, S=S/total, N=N/total)\n",
    "\n",
    "# Apply YRSN computation\n",
    "yrsn_results = requests_df.apply(compute_rag_yrsn, axis=1)\n",
    "requests_df['R'] = [y.R for y in yrsn_results]\n",
    "requests_df['S'] = [y.S for y in yrsn_results]\n",
    "requests_df['N'] = [y.N for y in yrsn_results]\n",
    "requests_df['collapse_type'] = [y.collapse_type.value for y in yrsn_results]\n",
    "\n",
    "print(\"YRSN for RAG Context:\")\n",
    "requests_df[['id', 'expected_category', 'R', 'S', 'N', 'collapse_type']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 7. Temperature-Based Routing for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute temperature\n",
    "requests_df['temperature'] = requests_df['R'].apply(lambda r: compute_temperature(r))\n",
    "\n",
    "# RAG-specific routing\n",
    "def route_rag_request(row):\n",
    "    tau = row['temperature']\n",
    "    collapse = row['collapse_type']\n",
    "    \n",
    "    # CONFUSION collapse: force red (wrong policy retrieved)\n",
    "    if collapse == 'confusion':\n",
    "        return 'red'\n",
    "    \n",
    "    # High temperature (low R): need human to verify/find policy\n",
    "    if tau > 2.0:\n",
    "        return 'red'\n",
    "    \n",
    "    # Medium temperature: human reviews retrieved context\n",
    "    elif tau > 1.3:\n",
    "        return 'yellow'\n",
    "    \n",
    "    # Low temperature: trust the context\n",
    "    else:\n",
    "        return 'green'\n",
    "\n",
    "requests_df['stream'] = requests_df.apply(route_rag_request, axis=1)\n",
    "\n",
    "print(\"RAG Routing Results:\")\n",
    "print(requests_df[['id', 'expected_category', 'R', 'temperature', 'stream', 'collapse_type']])\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(f\"Routing Distribution:\")\n",
    "print(requests_df['stream'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 8. Analyze OOD and CONFUSION Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOD Cases (novel requests)\n",
    "ood_cases = requests_df[requests_df['expected_category'] == 'OOD']\n",
    "\n",
    "print(\"OUT-OF-DISTRIBUTION Cases:\")\n",
    "print(\"=\"*60)\n",
    "for _, row in ood_cases.iterrows():\n",
    "    print(f\"\\n[{row['id']}] {row['text'][:60]}...\")\n",
    "    print(f\"  OOD Score: {row['ood_score_raw']:.3f} (higher = more OOD)\")\n",
    "    print(f\"  Top Match: {row['top_category']} (sim={row['top1_similarity']:.3f})\")\n",
    "    print(f\"  YRSN: R={row['R']:.2f}, S={row['S']:.2f}, N={row['N']:.2f}\")\n",
    "    print(f\"  Stream: {row['stream'].upper()}, Ï„={row['temperature']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFUSION Cases (category disagreement)\n",
    "confusion_cases = requests_df[~requests_df['category_agreement']]\n",
    "\n",
    "print(\"CONFUSION Cases (Category Disagreement in Retrieved Docs):\")\n",
    "print(\"=\"*60)\n",
    "for _, row in confusion_cases.iterrows():\n",
    "    req_results = retrieval_df[retrieval_df['request_id'] == row['id']]\n",
    "    categories = req_results['policy_category'].tolist()\n",
    "    \n",
    "    print(f\"\\n[{row['id']}] {row['text'][:60]}...\")\n",
    "    print(f\"  Retrieved categories: {categories}\")\n",
    "    print(f\"  YRSN: R={row['R']:.2f}, S={row['S']:.2f}, N={row['N']:.2f}\")\n",
    "    print(f\"  Collapse: {row['collapse_type']}\")\n",
    "    print(f\"  Stream: {row['stream'].upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 9. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "output_cols = ['id', 'text', 'expected_category', 'top_category',\n",
    "               'top1_similarity', 'category_agreement', 'in_distribution_score',\n",
    "               'R', 'S', 'N', 'collapse_type', 'temperature', 'stream']\n",
    "\n",
    "requests_df[output_cols].to_csv('rag_yrsn_results.csv', index=False)\n",
    "retrieval_df.to_csv('retrieval_results.csv', index=False)\n",
    "\n",
    "print(f\"Saved RAG analysis results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we:\n",
    "1. Created a policy document knowledge base with semantic search\n",
    "2. Used Cleanlab OOD detection to identify novel request types\n",
    "3. Computed retrieval quality metrics (similarity, category agreement)\n",
    "4. Mapped to YRSN: OOD â†’ N, Low similarity â†’ S, Category confusion â†’ N\n",
    "5. Applied temperature-based routing for RAG decisions\n",
    "6. Identified CONFUSION collapse cases (wrong policy retrieved)\n",
    "\n",
    "**Key Insight**: When retrieved context is uncertain (OOD query or low similarity), temperature increases, routing to yellow/red for human verification of policy applicability.\n",
    "\n",
    "**Next**: Notebook 5 - Token/NER Classification for Contract Entities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
