{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Notebook 6: Production Pipeline - Full IARS Integration\n",
    "\n",
    "**Objective**: Deploy a complete Intelligent Approval Routing System:\n",
    "- Integrate all Cleanlab functions via Datalab\n",
    "- Implement temperature-aware routing with annealing\n",
    "- Handle multiple collapse types simultaneously\n",
    "- Production monitoring and metrics\n",
    "\n",
    "---\n",
    "\n",
    "## Flow Diagram\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    subgraph Input[\"üì• Approval Request\"]\n",
    "        A[Request Text]\n",
    "        B[Amount/Category]\n",
    "        C[Requestor Info]\n",
    "    end\n",
    "\n",
    "    subgraph Embedding[\"üî§ Feature Extraction\"]\n",
    "        D[Text Embedding]\n",
    "        E[Structured Features]\n",
    "        F[Combined Vector]\n",
    "    end\n",
    "\n",
    "    subgraph Classifier[\"ü§ñ ML Classification\"]\n",
    "        G[Category Classifier]\n",
    "        H[Prediction Probs]\n",
    "        I[Confidence Score]\n",
    "    end\n",
    "\n",
    "    subgraph Datalab[\"üßπ Cleanlab Datalab\"]\n",
    "        J[Datalab.find_issues]\n",
    "        K[Label Quality]\n",
    "        L[OOD Score]\n",
    "        M[Near Duplicates]\n",
    "        N[Dataset Health]\n",
    "    end\n",
    "\n",
    "    subgraph YRSN[\"üéØ YRSN Decomposition\"]\n",
    "        O[CleanlabAdapter]\n",
    "        P[R Component]\n",
    "        Q[S Component]\n",
    "        R2[N Component]\n",
    "        S2[Collapse Detection]\n",
    "    end\n",
    "\n",
    "    subgraph Urgency[\"‚è∞ Urgency Scoring\"]\n",
    "        T[T_expiry]\n",
    "        U[P_business]\n",
    "        V[S_sentiment]\n",
    "        W[C_clarity]\n",
    "        X[Weighted Sum]\n",
    "    end\n",
    "\n",
    "    subgraph Temperature[\"üå°Ô∏è Temperature Routing\"]\n",
    "        Y[Compute œÑ = 1/Œ±]\n",
    "        Z[Annealing Schedule]\n",
    "        AA[Threshold Adjustment]\n",
    "    end\n",
    "\n",
    "    subgraph Routing[\"üö¶ Stream Decision\"]\n",
    "        AB{Knockout Rules?}\n",
    "        AC{Collapse Type?}\n",
    "        AD{Confidence + YRSN?}\n",
    "        AE[üü¢ GREEN: Auto-process]\n",
    "        AF[üü° YELLOW: AI-assisted]\n",
    "        AG[üî¥ RED: Expert review]\n",
    "    end\n",
    "\n",
    "    subgraph Output[\"üì§ Decision\"]\n",
    "        AH[RoutingDecision]\n",
    "        AI[Soft Probabilities]\n",
    "        AJ[Urgency Level]\n",
    "        AK[Audit Trail]\n",
    "    end\n",
    "\n",
    "    subgraph Monitoring[\"üìä Production Metrics\"]\n",
    "        AL[Automation Rate]\n",
    "        AM[Temperature Distribution]\n",
    "        AN[Collapse Frequency]\n",
    "        AO[Quality Trends]\n",
    "    end\n",
    "\n",
    "    A --> D\n",
    "    B --> E\n",
    "    C --> E\n",
    "    D --> F\n",
    "    E --> F\n",
    "    F --> G\n",
    "    G --> H\n",
    "    H --> I\n",
    "    H --> J\n",
    "    J --> K\n",
    "    J --> L\n",
    "    J --> M\n",
    "    J --> N\n",
    "    K --> O\n",
    "    L --> O\n",
    "    M --> O\n",
    "    O --> P\n",
    "    O --> Q\n",
    "    O --> R2\n",
    "    O --> S2\n",
    "    C --> T\n",
    "    B --> U\n",
    "    A --> V\n",
    "    P --> W\n",
    "    R2 --> W\n",
    "    T --> X\n",
    "    U --> X\n",
    "    V --> X\n",
    "    W --> X\n",
    "    P --> Y\n",
    "    Y --> Z\n",
    "    Z --> AA\n",
    "    I --> AB\n",
    "    AB -->|Yes| AG\n",
    "    AB -->|No| AC\n",
    "    S2 --> AC\n",
    "    AC -->|POISONING/CONFUSION| AG\n",
    "    AC -->|DISTRACTION| AF\n",
    "    AC -->|NONE/CLASH| AD\n",
    "    AA --> AD\n",
    "    AD -->|High Cs, Low œÑ| AE\n",
    "    AD -->|Medium| AF\n",
    "    AD -->|Low Cs, High œÑ| AG\n",
    "    AE --> AH\n",
    "    AF --> AH\n",
    "    AG --> AH\n",
    "    AH --> AI\n",
    "    X --> AJ\n",
    "    AH --> AK\n",
    "    AH --> AL\n",
    "    Y --> AM\n",
    "    S2 --> AN\n",
    "    N --> AO\n",
    "\n",
    "    style Datalab fill:#e1f5fe\n",
    "    style YRSN fill:#e8f5e9\n",
    "    style Temperature fill:#fff3e0\n",
    "    style Routing fill:#fce4ec\n",
    "    style Monitoring fill:#f3e5f5\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Collapse Types Handled**: ALL (POISONING, DISTRACTION, CONFUSION, CLASH)\n",
    "\n",
    "**Difficulty**: ‚≠ê‚≠ê‚≠ê Hard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install cleanlab sentence-transformers scikit-learn pandas numpy --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Cleanlab Datalab (unified interface)\n",
    "from cleanlab import Datalab\n",
    "from cleanlab.dataset import (\n",
    "    overall_label_health_score,\n",
    "    rank_classes_by_label_quality,\n",
    "    find_overlapping_classes\n",
    ")\n",
    "from cleanlab.filter import get_confident_thresholds\n",
    "\n",
    "# Import YRSN-IARS modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from yrsn_iars.adapters.cleanlab_adapter import CleanlabAdapter, YRSNResult, CollapseType\n",
    "from yrsn_iars.adapters.temperature import (\n",
    "    TemperatureScheduler, TemperatureConfig, TemperatureMode,\n",
    "    AdaptiveRoutingEngine, compute_temperature\n",
    ")\n",
    "from yrsn_iars.pipelines.approval_router import (\n",
    "    ApprovalRouter, ApprovalRequest, RoutingDecision, \n",
    "    Stream, UrgencyLevel, UrgencyWeights\n",
    ")\n",
    "\n",
    "print(\"All dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Generate Production-Scale Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_samples = 2000\n",
    "\n",
    "# Realistic approval categories\n",
    "categories = [\n",
    "    'software_license', 'travel_request', 'budget_variance',\n",
    "    'vendor_payment', 'equipment_purchase', 'training_expense',\n",
    "    'contractor_invoice', 'marketing_spend', 'office_supplies',\n",
    "    'professional_services'\n",
    "]\n",
    "\n",
    "# Generate diverse request data\n",
    "data = []\n",
    "for i in range(n_samples):\n",
    "    category = np.random.choice(categories)\n",
    "    \n",
    "    # Amount distribution varies by category\n",
    "    if category in ['budget_variance', 'vendor_payment']:\n",
    "        amount = np.random.exponential(50000)\n",
    "    elif category in ['software_license', 'professional_services']:\n",
    "        amount = np.random.exponential(15000)\n",
    "    elif category == 'office_supplies':\n",
    "        amount = np.random.exponential(500)\n",
    "    else:\n",
    "        amount = np.random.exponential(5000)\n",
    "    \n",
    "    amount = max(100, min(500000, amount))  # Clamp\n",
    "    \n",
    "    # Text templates\n",
    "    templates = {\n",
    "        'software_license': f\"Request for {np.random.choice(['AWS', 'Azure', 'Salesforce', 'Adobe', 'Slack'])} license - ${amount:,.0f}\",\n",
    "        'travel_request': f\"Travel to {np.random.choice(['NYC', 'SF', 'London', 'Tokyo', 'Dubai'])} for {np.random.choice(['client meeting', 'conference', 'training'])}\",\n",
    "        'budget_variance': f\"Q{np.random.randint(1,5)} budget adjustment of ${amount:,.0f} for {np.random.choice(['project overrun', 'new initiative', 'unforeseen costs'])}\",\n",
    "        'vendor_payment': f\"Payment to {np.random.choice(['Acme Corp', 'Tech Solutions', 'Global Services'])} - Invoice #{np.random.randint(10000, 99999)}\",\n",
    "        'equipment_purchase': f\"Purchase of {np.random.choice(['laptops', 'monitors', 'servers', 'network equipment'])} for {np.random.choice(['new hires', 'refresh', 'expansion'])}\",\n",
    "        'training_expense': f\"{np.random.choice(['AWS Certification', 'Leadership Training', 'Technical Workshop'])} for {np.random.randint(1, 20)} employees\",\n",
    "        'contractor_invoice': f\"Contractor {np.random.choice(['development', 'consulting', 'design'])} services - {np.random.randint(40, 200)} hours\",\n",
    "        'marketing_spend': f\"{np.random.choice(['Digital ads', 'Trade show', 'Content creation'])} campaign Q{np.random.randint(1,5)}\",\n",
    "        'office_supplies': f\"Office supplies order - {np.random.choice(['paper', 'printer ink', 'ergonomic accessories'])}\",\n",
    "        'professional_services': f\"{np.random.choice(['Legal', 'Accounting', 'HR Consulting'])} services engagement\"\n",
    "    }\n",
    "    \n",
    "    # Decision (with some noise)\n",
    "    # Base approval rate varies by category\n",
    "    base_rates = {\n",
    "        'office_supplies': 0.95,\n",
    "        'training_expense': 0.85,\n",
    "        'software_license': 0.80,\n",
    "        'travel_request': 0.75,\n",
    "        'equipment_purchase': 0.70,\n",
    "        'marketing_spend': 0.65,\n",
    "        'contractor_invoice': 0.70,\n",
    "        'professional_services': 0.65,\n",
    "        'vendor_payment': 0.80,\n",
    "        'budget_variance': 0.50\n",
    "    }\n",
    "    \n",
    "    # Amount affects approval\n",
    "    amount_modifier = -0.1 * (amount / 100000)  # Higher amounts less likely approved\n",
    "    approval_prob = base_rates[category] + amount_modifier\n",
    "    decision = 1 if np.random.random() < approval_prob else 0\n",
    "    \n",
    "    # Deadline (some urgent, some not)\n",
    "    if np.random.random() < 0.2:\n",
    "        deadline = datetime.now() + timedelta(hours=np.random.randint(2, 48))\n",
    "    elif np.random.random() < 0.5:\n",
    "        deadline = datetime.now() + timedelta(days=np.random.randint(1, 14))\n",
    "    else:\n",
    "        deadline = None\n",
    "    \n",
    "    data.append({\n",
    "        'request_id': f'REQ-{i:05d}',\n",
    "        'text': templates[category],\n",
    "        'category': category,\n",
    "        'amount': amount,\n",
    "        'decision': decision,\n",
    "        'deadline': deadline,\n",
    "        'requestor_level': np.random.randint(1, 8),\n",
    "        'requestor_id': f'EMP-{np.random.randint(1000, 9999)}'\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Inject label noise (realistic)\n",
    "noise_rate = 0.08\n",
    "noise_indices = np.random.choice(n_samples, size=int(n_samples * noise_rate), replace=False)\n",
    "df.loc[noise_indices, 'decision'] = 1 - df.loc[noise_indices, 'decision']\n",
    "\n",
    "# Inject some near-duplicates\n",
    "for _ in range(50):\n",
    "    source_idx = np.random.randint(0, n_samples)\n",
    "    duplicate = df.iloc[source_idx].copy()\n",
    "    duplicate['request_id'] = f'REQ-DUP-{np.random.randint(10000, 99999)}'\n",
    "    duplicate['text'] = duplicate['text'] + \" (duplicate request)\"\n",
    "    df = pd.concat([df, pd.DataFrame([duplicate])], ignore_index=True)\n",
    "\n",
    "print(f\"Generated {len(df)} approval requests\")\n",
    "print(f\"\\nCategory distribution:\")\n",
    "print(df['category'].value_counts())\n",
    "print(f\"\\nDecision distribution: {df['decision'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering & Classifier Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text embeddings\n",
    "print(\"Loading embedding model...\")\n",
    "encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(\"Generating embeddings...\")\n",
    "text_embeddings = encoder.encode(df['text'].tolist(), show_progress_bar=True)\n",
    "\n",
    "# Structured features\n",
    "scaler = StandardScaler()\n",
    "structured_features = scaler.fit_transform(\n",
    "    df[['amount', 'requestor_level']].values\n",
    ")\n",
    "\n",
    "# Combine features\n",
    "X = np.hstack([text_embeddings, structured_features])\n",
    "\n",
    "# Encode labels\n",
    "y = df['decision'].values\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifier with cross-validation\n",
    "print(\"Training classifier...\")\n",
    "clf = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "\n",
    "pred_probs = cross_val_predict(\n",
    "    clf,\n",
    "    X, y,\n",
    "    cv=5,\n",
    "    method='predict_proba',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train final model for inference\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(f\"Cross-validation complete\")\n",
    "print(f\"Prediction shape: {pred_probs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. Cleanlab Datalab Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Datalab for unified issue detection\n",
    "datalab_data = {\n",
    "    'text': df['text'].tolist(),\n",
    "    'label': df['decision'].tolist()\n",
    "}\n",
    "\n",
    "print(\"Running Datalab analysis...\")\n",
    "lab = Datalab(data=datalab_data, label_name='label')\n",
    "\n",
    "# Find all issues\n",
    "lab.find_issues(\n",
    "    pred_probs=pred_probs,\n",
    "    features=text_embeddings  # For outlier/duplicate detection\n",
    ")\n",
    "\n",
    "print(\"\\nDatalab Issue Summary:\")\n",
    "lab.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all Datalab signals\n",
    "issues_df = lab.get_issues()\n",
    "\n",
    "print(f\"Datalab columns: {issues_df.columns.tolist()}\")\n",
    "print(f\"\\nIssue statistics:\")\n",
    "for col in issues_df.columns:\n",
    "    if col.startswith('is_'):\n",
    "        print(f\"  {col}: {issues_df[col].sum()} issues\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset-level health\n",
    "health_score = overall_label_health_score(y, pred_probs)\n",
    "print(f\"\\nOverall Dataset Health Score: {health_score:.3f}\")\n",
    "\n",
    "# Class quality\n",
    "class_quality = rank_classes_by_label_quality(\n",
    "    labels=y,\n",
    "    pred_probs=pred_probs,\n",
    "    class_names=['Reject', 'Approve']\n",
    ")\n",
    "print(f\"\\nClass Quality:\")\n",
    "print(class_quality)\n",
    "\n",
    "# Confident thresholds\n",
    "thresholds = get_confident_thresholds(y, pred_probs)\n",
    "print(f\"\\nConfident Thresholds: {thresholds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 5. YRSN Decomposition via Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize adapter\n",
    "adapter = CleanlabAdapter()\n",
    "\n",
    "# Convert Datalab results to YRSN\n",
    "yrsn_df = adapter.from_datalab(lab, include_details=True)\n",
    "\n",
    "# Merge with original data\n",
    "df = df.reset_index(drop=True)\n",
    "df = pd.concat([df, yrsn_df[['R', 'S', 'N', 'quality_score', 'risk_score', 'collapse_type']]], axis=1)\n",
    "\n",
    "print(\"YRSN Statistics:\")\n",
    "print(df[['R', 'S', 'N']].describe())\n",
    "\n",
    "print(f\"\\nCollapse Type Distribution:\")\n",
    "print(df['collapse_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 6. Initialize Production Router with Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure temperature with annealing (production warm-up)\n",
    "temp_config = TemperatureConfig(\n",
    "    mode=TemperatureMode.ANNEALING,\n",
    "    tau_initial=2.0,      # Start loose (more human review)\n",
    "    tau_final=0.5,        # End tight (more automation)\n",
    "    annealing_steps=500,  # Steps to full automation\n",
    "    tau_min=0.3,\n",
    "    tau_max=3.0\n",
    ")\n",
    "\n",
    "# Initialize router\n",
    "router = ApprovalRouter(\n",
    "    cleanlab_adapter=adapter,\n",
    "    temperature_config=temp_config,\n",
    "    knockout_rules=[\n",
    "        {\n",
    "            \"rule\": \"amount_exceeds_limit\",\n",
    "            \"condition\": lambda r: r.amount > 250000,\n",
    "            \"message\": \"Amount exceeds $250K limit\",\n",
    "            \"action\": \"ROUTE_TO_EXPERT\"\n",
    "        },\n",
    "        {\n",
    "            \"rule\": \"auto_approve_supplies\",\n",
    "            \"condition\": lambda r: r.amount < 200 and r.category == \"office_supplies\",\n",
    "            \"message\": \"Below auto-approval threshold\",\n",
    "            \"action\": \"APPROVE\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Router initialized with ANNEALING temperature schedule\")\n",
    "print(f\"Initial œÑ: {temp_config.tau_initial}, Final œÑ: {temp_config.tau_final}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 7. Process All Requests Through Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each request\n",
    "decisions = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    # Create ApprovalRequest\n",
    "    request = ApprovalRequest(\n",
    "        request_id=row['request_id'],\n",
    "        text=row['text'],\n",
    "        category=row['category'],\n",
    "        amount=row['amount'],\n",
    "        requestor_id=row['requestor_id'],\n",
    "        deadline=row['deadline'],\n",
    "        requestor_level=row['requestor_level']\n",
    "    )\n",
    "    \n",
    "    # Get label quality from Datalab\n",
    "    label_quality = issues_df.loc[idx, 'label_score'] if 'label_score' in issues_df.columns else 0.8\n",
    "    \n",
    "    # Get OOD score\n",
    "    ood_score = 1 - issues_df.loc[idx, 'outlier_score'] if 'outlier_score' in issues_df.columns else 0.9\n",
    "    \n",
    "    # Get duplicate flag\n",
    "    is_duplicate = issues_df.loc[idx, 'is_near_duplicate_issue'] if 'is_near_duplicate_issue' in issues_df.columns else False\n",
    "    \n",
    "    # Classifier confidence\n",
    "    classifier_conf = pred_probs[idx].max()\n",
    "    \n",
    "    # Route request\n",
    "    decision = router.route(\n",
    "        request=request,\n",
    "        classifier_confidence=classifier_conf,\n",
    "        label_quality=label_quality,\n",
    "        pred_probs=pred_probs[idx],\n",
    "        ood_score=ood_score,\n",
    "        is_duplicate=is_duplicate\n",
    "    )\n",
    "    \n",
    "    decisions.append(decision)\n",
    "    \n",
    "    # Progress\n",
    "    if (idx + 1) % 500 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(df)} requests\")\n",
    "\n",
    "print(f\"\\nRouting complete: {len(decisions)} decisions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 8. Production Metrics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert decisions to DataFrame\n",
    "decisions_df = pd.DataFrame([d.to_dict() for d in decisions])\n",
    "\n",
    "# Add to original data\n",
    "df['stream'] = decisions_df['stream'].values\n",
    "df['urgency'] = decisions_df['urgency'].values\n",
    "df['temperature'] = decisions_df['temperature'].values\n",
    "df['confidence_score'] = decisions_df['confidence_score'].values\n",
    "df['p_green'] = decisions_df['p_green'].values\n",
    "df['p_yellow'] = decisions_df['p_yellow'].values\n",
    "df['p_red'] = decisions_df['p_red'].values\n",
    "\n",
    "# Get router metrics\n",
    "metrics = router.get_metrics()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PRODUCTION ROUTING METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal Decisions: {metrics['n_decisions']}\")\n",
    "print(f\"\\nStream Distribution:\")\n",
    "print(f\"  üü¢ GREEN (Auto):    {metrics['automation_rate']*100:.1f}%\")\n",
    "print(f\"  üü° YELLOW (Assist): {metrics['assisted_rate']*100:.1f}%\")\n",
    "print(f\"  üî¥ RED (Expert):    {metrics['expert_rate']*100:.1f}%\")\n",
    "print(f\"\\nQuality Metrics:\")\n",
    "print(f\"  Avg Confidence: {metrics['avg_confidence']:.3f}\")\n",
    "print(f\"  Avg Temperature: {metrics['avg_temperature']:.3f}\")\n",
    "print(f\"  Avg Quality (Œ±): {metrics['avg_quality_alpha']:.3f}\")\n",
    "print(f\"\\nSpecial Cases:\")\n",
    "print(f\"  Knockout Rate: {metrics['knockout_rate']*100:.1f}%\")\n",
    "print(f\"  Collapse Rate: {metrics['collapse_rate']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature explanation\n",
    "print(router.explain_temperature())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze by category\n",
    "category_routing = df.groupby('category').agg({\n",
    "    'stream': lambda x: (x == 'green').mean(),\n",
    "    'temperature': 'mean',\n",
    "    'R': 'mean',\n",
    "    'N': 'mean',\n",
    "    'amount': 'mean'\n",
    "}).round(3)\n",
    "category_routing.columns = ['automation_rate', 'avg_temp', 'avg_R', 'avg_N', 'avg_amount']\n",
    "\n",
    "print(\"\\nRouting by Category:\")\n",
    "print(category_routing.sort_values('automation_rate', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 9. Collapse Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze each collapse type\n",
    "collapse_analysis = df.groupby('collapse_type').agg({\n",
    "    'stream': lambda x: pd.Series({'green': (x=='green').mean(), \n",
    "                                   'yellow': (x=='yellow').mean(),\n",
    "                                   'red': (x=='red').mean()}).to_dict(),\n",
    "    'R': 'mean',\n",
    "    'S': 'mean',\n",
    "    'N': 'mean',\n",
    "    'temperature': 'mean'\n",
    "})\n",
    "\n",
    "print(\"\\nCollapse Type Analysis:\")\n",
    "print(\"=\"*60)\n",
    "for collapse_type in df['collapse_type'].unique():\n",
    "    subset = df[df['collapse_type'] == collapse_type]\n",
    "    print(f\"\\n{collapse_type.upper()}:\")\n",
    "    print(f\"  Count: {len(subset)}\")\n",
    "    print(f\"  Avg YRSN: R={subset['R'].mean():.2f}, S={subset['S'].mean():.2f}, N={subset['N'].mean():.2f}\")\n",
    "    print(f\"  Avg œÑ: {subset['temperature'].mean():.2f}\")\n",
    "    print(f\"  Stream: GREEN={100*(subset['stream']=='green').mean():.0f}%, \"\n",
    "          f\"YELLOW={100*(subset['stream']=='yellow').mean():.0f}%, \"\n",
    "          f\"RED={100*(subset['stream']=='red').mean():.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## 10. Urgency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Urgency analysis\n",
    "print(\"\\nUrgency Distribution:\")\n",
    "print(df['urgency'].value_counts())\n",
    "\n",
    "# Urgency by stream\n",
    "print(\"\\nUrgency by Stream:\")\n",
    "print(pd.crosstab(df['stream'], df['urgency'], normalize='index').round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## 11. Sample Routing Decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample decisions from each stream\n",
    "for stream in ['green', 'yellow', 'red']:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{stream.upper()} STREAM SAMPLES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    samples = df[df['stream'] == stream].head(3)\n",
    "    for _, row in samples.iterrows():\n",
    "        print(f\"\\n[{row['request_id']}] {row['text'][:60]}...\")\n",
    "        print(f\"  Category: {row['category']}, Amount: ${row['amount']:,.0f}\")\n",
    "        print(f\"  YRSN: R={row['R']:.2f}, S={row['S']:.2f}, N={row['N']:.2f}\")\n",
    "        print(f\"  Temperature: œÑ={row['temperature']:.2f}\")\n",
    "        print(f\"  Confidence: {row['confidence_score']:.3f}\")\n",
    "        print(f\"  Urgency: {row['urgency']}\")\n",
    "        print(f\"  Soft Probs: G={row['p_green']:.2f}, Y={row['p_yellow']:.2f}, R={row['p_red']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## 12. Export Production Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export routing decisions\n",
    "output_cols = ['request_id', 'text', 'category', 'amount', 'decision',\n",
    "               'R', 'S', 'N', 'collapse_type', 'temperature', \n",
    "               'confidence_score', 'stream', 'urgency',\n",
    "               'p_green', 'p_yellow', 'p_red']\n",
    "\n",
    "df[output_cols].to_csv('production_routing_results.csv', index=False)\n",
    "\n",
    "# Export metrics summary\n",
    "metrics_summary = {\n",
    "    **metrics,\n",
    "    'dataset_health': health_score,\n",
    "    'n_categories': df['category'].nunique(),\n",
    "    'temperature_mode': 'ANNEALING',\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "pd.DataFrame([metrics_summary]).to_csv('production_metrics.csv', index=False)\n",
    "\n",
    "print(f\"Exported {len(df)} routing decisions\")\n",
    "print(f\"Exported production metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we:\n",
    "1. Generated a production-scale dataset (2000+ requests across 10 categories)\n",
    "2. Trained a classifier with cross-validation\n",
    "3. Used Cleanlab Datalab for comprehensive issue detection\n",
    "4. Converted all signals to YRSN via CleanlabAdapter\n",
    "5. Initialized ApprovalRouter with ANNEALING temperature schedule\n",
    "6. Processed all requests and analyzed production metrics\n",
    "7. Examined routing patterns by category and collapse type\n",
    "8. Analyzed urgency distribution across streams\n",
    "\n",
    "**Key Insights**:\n",
    "- Temperature annealing allows gradual transition from conservative to automated routing\n",
    "- Different categories have different natural automation rates based on data quality\n",
    "- Collapse types (POISONING, CONFUSION) properly route to RED stream\n",
    "- The œÑ = 1/Œ± relationship ensures system self-calibrates based on quality\n",
    "\n",
    "---\n",
    "\n",
    "## Production Deployment Checklist\n",
    "\n",
    "- [ ] Configure AWS Bedrock for embeddings\n",
    "- [ ] Set up SageMaker endpoint for classifier\n",
    "- [ ] Deploy Datalab analysis as Lambda function\n",
    "- [ ] Configure DynamoDB for decision audit trail\n",
    "- [ ] Set up CloudWatch metrics for monitoring\n",
    "- [ ] Configure SNS alerts for high collapse rates\n",
    "- [ ] Implement A/B testing infrastructure\n",
    "- [ ] Set up periodic model retraining pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
