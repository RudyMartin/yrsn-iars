{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Notebook 1: Cleanlab for Approval Data Quality\n\n**Objective**: Use Cleanlab to audit historical approval decisions and identify:\n- Mislabeled approvals/rejections\n- Inconsistent approver decisions\n- Ambiguous cases that were decided inconsistently\n\nThis prepares clean training data for the IARS classifier.\n\n---\n\n## Flow Diagram\n\n```mermaid\nflowchart TD\n    subgraph Input[\"ðŸ“¥ Input Data\"]\n        A[Historical Approval Data]\n        B[Request Text + Decisions]\n    end\n\n    subgraph Embeddings[\"ðŸ”¤ Text Processing\"]\n        C[Sentence Transformer]\n        D[Text Embeddings]\n    end\n\n    subgraph ML[\"ðŸ¤– Machine Learning\"]\n        E[Train Classifier]\n        F[Cross-Validated Predictions]\n        G[Prediction Probabilities]\n    end\n\n    subgraph Cleanlab[\"ðŸ§¹ Cleanlab Analysis\"]\n        H[get_label_quality_scores]\n        I[find_label_issues]\n        J[compute_confident_joint]\n        K[health_summary]\n    end\n\n    subgraph Output[\"ðŸ“¤ Outputs\"]\n        L[Label Quality Scores]\n        M[Issue Flags]\n        N[Dataset Health]\n        O[Cleaned Training Data]\n    end\n\n    subgraph YRSN[\"ðŸŽ¯ YRSN Signals\"]\n        P[label_quality â†’ N component]\n        Q[normalized_margin â†’ S component]\n        R[normalized_entropy â†’ S component]\n    end\n\n    A --> C\n    B --> C\n    C --> D\n    D --> E\n    E --> F\n    F --> G\n    G --> H\n    G --> I\n    G --> J\n    G --> K\n    H --> L\n    I --> M\n    J --> N\n    K --> N\n    L --> O\n    M --> O\n    L --> P\n    G --> Q\n    G --> R\n\n    style Cleanlab fill:#e1f5fe\n    style YRSN fill:#fff3e0\n    style Input fill:#e8f5e9\n    style Output fill:#fce4ec\n```\n\n---\n\n**Collapse Type Focus**: POISONING (mislabeled approval decisions)\n\n**Difficulty**: â­ Easy"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install cleanlab sentence-transformers scikit-learn pandas boto3 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from cleanlab.filter import find_label_issues\n",
    "from cleanlab.rank import get_label_quality_scores\n",
    "from cleanlab.count import compute_confident_joint\n",
    "from cleanlab.dataset import health_summary\n",
    "\n",
    "print(\"Dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Historical Approval Data\n",
    "\n",
    "Replace with your actual data source (S3, database, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Load from S3\n",
    "# import boto3\n",
    "# s3 = boto3.client('s3')\n",
    "# df = pd.read_csv('s3://your-bucket/approval_history.csv')\n",
    "\n",
    "# Option B: Load from local file\n",
    "# df = pd.read_csv('approval_history.csv')\n",
    "\n",
    "# Option C: Generate synthetic data for demo\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Synthetic approval request data\n",
    "categories = ['software_license', 'travel_request', 'budget_variance', \n",
    "              'vendor_payment', 'pto_request', 'equipment_purchase']\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'request_id': range(n_samples),\n",
    "    'request_text': [\n",
    "        f\"Request for {np.random.choice(categories)} - Amount: ${np.random.randint(100, 10000)}. \"\n",
    "        f\"Justification: {'Business critical' if np.random.random() > 0.5 else 'Standard request'}.\"\n",
    "        for _ in range(n_samples)\n",
    "    ],\n",
    "    'category': np.random.choice(categories, n_samples),\n",
    "    'amount': np.random.randint(100, 10000, n_samples),\n",
    "    'decision': np.random.choice([0, 1], n_samples, p=[0.3, 0.7]),  # 0=reject, 1=approve\n",
    "    'approver_id': np.random.choice(['approver_1', 'approver_2', 'approver_3'], n_samples)\n",
    "})\n",
    "\n",
    "# Inject some label noise (realistic scenario)\n",
    "noise_indices = np.random.choice(n_samples, size=int(n_samples * 0.1), replace=False)\n",
    "df.loc[noise_indices, 'decision'] = 1 - df.loc[noise_indices, 'decision']\n",
    "\n",
    "print(f\"Loaded {len(df)} historical approval decisions\")\n",
    "print(f\"\\nDecision distribution:\")\n",
    "print(df['decision'].value_counts())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sentence transformer for text embeddings\n",
    "print(\"Loading embedding model...\")\n",
    "encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Embed request text\n",
    "print(\"Generating embeddings...\")\n",
    "embeddings = encoder.encode(\n",
    "    df['request_text'].tolist(),\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Classifier and Get Cross-Validated Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifier\n",
    "clf = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "\n",
    "# Get out-of-fold predicted probabilities (critical for Cleanlab)\n",
    "print(\"Getting cross-validated predictions...\")\n",
    "pred_probs = cross_val_predict(\n",
    "    clf,\n",
    "    embeddings,\n",
    "    df['decision'].values,\n",
    "    cv=5,\n",
    "    method='predict_proba',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"Prediction shape: {pred_probs.shape}\")\n",
    "print(f\"Sample predictions: {pred_probs[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cleanlab Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get label quality scores\n",
    "labels = df['decision'].values\n",
    "\n",
    "label_quality = get_label_quality_scores(labels, pred_probs)\n",
    "df['label_quality'] = label_quality\n",
    "\n",
    "print(f\"Label quality statistics:\")\n",
    "print(f\"  Mean: {label_quality.mean():.3f}\")\n",
    "print(f\"  Std:  {label_quality.std():.3f}\")\n",
    "print(f\"  Min:  {label_quality.min():.3f}\")\n",
    "print(f\"  Max:  {label_quality.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find label issues\n",
    "label_issues = find_label_issues(labels, pred_probs)\n",
    "df['is_label_issue'] = label_issues\n",
    "\n",
    "print(f\"\\nFound {label_issues.sum()} potential label issues ({100*label_issues.sum()/len(labels):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confident joint matrix\n",
    "confident_joint = compute_confident_joint(labels, pred_probs)\n",
    "\n",
    "print(\"Confident Joint Matrix (rows=given label, cols=estimated true label):\")\n",
    "print(pd.DataFrame(\n",
    "    confident_joint,\n",
    "    index=['Given: Reject', 'Given: Approve'],\n",
    "    columns=['True: Reject', 'True: Approve']\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset health summary\n",
    "health = health_summary(labels, pred_probs)\n",
    "\n",
    "print(\"\\nDataset Health Summary:\")\n",
    "print(f\"  Overall health score: {health['overall_label_health_score']:.3f}\")\n",
    "print(f\"  Number of label issues: {health['num_label_issues']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Examine Problematic Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get worst quality examples\n",
    "worst_examples = df.nsmallest(20, 'label_quality')[\n",
    "    ['request_id', 'request_text', 'category', 'amount', 'decision', \n",
    "     'approver_id', 'label_quality', 'is_label_issue']\n",
    "]\n",
    "\n",
    "print(\"Top 20 Most Likely Mislabeled Decisions:\")\n",
    "print(\"=\"*80)\n",
    "for idx, row in worst_examples.iterrows():\n",
    "    decision_str = 'APPROVED' if row['decision'] == 1 else 'REJECTED'\n",
    "    print(f\"\\n[Quality: {row['label_quality']:.3f}] Request #{row['request_id']}\")\n",
    "    print(f\"  Category: {row['category']}, Amount: ${row['amount']}\")\n",
    "    print(f\"  Decision: {decision_str} by {row['approver_id']}\")\n",
    "    print(f\"  Text: {row['request_text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Approver Consistency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze approver quality\n",
    "approver_stats = df.groupby('approver_id').agg({\n",
    "    'label_quality': ['mean', 'std', 'count'],\n",
    "    'is_label_issue': 'sum'\n",
    "}).round(3)\n",
    "\n",
    "approver_stats.columns = ['avg_quality', 'std_quality', 'n_decisions', 'n_issues']\n",
    "approver_stats['issue_rate'] = (approver_stats['n_issues'] / approver_stats['n_decisions']).round(3)\n",
    "\n",
    "print(\"Approver Quality Analysis:\")\n",
    "print(approver_stats.sort_values('avg_quality'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clean training set (exclude label issues)\n",
    "df_clean = df[~df['is_label_issue']].copy()\n",
    "\n",
    "print(f\"Original dataset: {len(df)} examples\")\n",
    "print(f\"Clean dataset: {len(df_clean)} examples\")\n",
    "print(f\"Removed: {len(df) - len(df_clean)} noisy examples\")\n",
    "\n",
    "# Save for next notebook\n",
    "df_clean.to_csv('approval_data_cleaned.csv', index=False)\n",
    "np.save('embeddings_cleaned.npy', embeddings[~df['is_label_issue']])\n",
    "\n",
    "print(\"\\nCleaned data saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Signals for YRSN Integration\n",
    "\n",
    "These Cleanlab outputs will be used in the next notebook for YRSN decomposition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare YRSN input signals\n",
    "cleanlab_signals = pd.DataFrame({\n",
    "    'request_id': df['request_id'],\n",
    "    'label_quality': df['label_quality'],\n",
    "    'is_label_issue': df['is_label_issue'],\n",
    "    'pred_prob_reject': pred_probs[:, 0],\n",
    "    'pred_prob_approve': pred_probs[:, 1],\n",
    "})\n",
    "\n",
    "# Add derived signals\n",
    "cleanlab_signals['normalized_margin'] = np.abs(pred_probs[:, 1] - pred_probs[:, 0])\n",
    "cleanlab_signals['normalized_entropy'] = -np.sum(\n",
    "    pred_probs * np.log(pred_probs + 1e-10), axis=1\n",
    ") / np.log(2)  # Normalize by max entropy for binary\n",
    "\n",
    "cleanlab_signals.to_csv('cleanlab_signals.csv', index=False)\n",
    "\n",
    "print(\"Cleanlab signals for YRSN:\")\n",
    "cleanlab_signals.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we:\n",
    "1. Loaded historical approval data\n",
    "2. Generated text embeddings\n",
    "3. Ran Cleanlab analysis to find label issues\n",
    "4. Analyzed approver consistency\n",
    "5. Exported clean training data\n",
    "6. Prepared signals for YRSN decomposition\n",
    "\n",
    "**Next**: Notebook 2 - YRSN Decomposition for Approval Context"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}