{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Notebook 3: Multi-Annotator Consensus for Committee Decisions\n",
    "\n",
    "**Objective**: Handle approval scenarios with multiple approvers (committee decisions):\n",
    "- Find consensus among disagreeing annotators\n",
    "- Identify problematic approvers (always yes/no)\n",
    "- Map annotator agreement to YRSN components\n",
    "\n",
    "---\n",
    "\n",
    "## Flow Diagram\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    subgraph Input[\"üì• Committee Decisions\"]\n",
    "        A[Request Data]\n",
    "        B[Multiple Approver Votes]\n",
    "        C[Final Decision]\n",
    "    end\n",
    "\n",
    "    subgraph Cleanlab[\"üßπ Cleanlab Multi-Annotator\"]\n",
    "        D[get_label_quality_multiannotator]\n",
    "        E[get_majority_vote_label]\n",
    "        F[get_active_learning_scores]\n",
    "        G[Annotator Agreement Matrix]\n",
    "    end\n",
    "\n",
    "    subgraph Analysis[\"üîç Annotator Analysis\"]\n",
    "        H[Per-Annotator Quality]\n",
    "        I[Agreement Patterns]\n",
    "        J[Controversial Cases]\n",
    "    end\n",
    "\n",
    "    subgraph YRSN[\"üéØ YRSN Mapping\"]\n",
    "        K{Agreement Level?}\n",
    "        L[High Agreement ‚Üí R High]\n",
    "        M[Low Agreement, Good Consensus ‚Üí S Moderate]\n",
    "        N[Low Agreement, Bad Consensus ‚Üí N High]\n",
    "        O[CLASH Collapse Type]\n",
    "    end\n",
    "\n",
    "    subgraph Routing[\"üö¶ Temperature Routing\"]\n",
    "        P[Compute œÑ from Consensus Quality]\n",
    "        Q{Stream Decision}\n",
    "        R[üü¢ GREEN: Clear consensus]\n",
    "        S[üü° YELLOW: Needs tie-breaker]\n",
    "        T[üî¥ RED: Escalate to senior]\n",
    "    end\n",
    "\n",
    "    A --> D\n",
    "    B --> D\n",
    "    B --> E\n",
    "    B --> F\n",
    "    D --> G\n",
    "    G --> H\n",
    "    G --> I\n",
    "    G --> J\n",
    "    D --> K\n",
    "    I --> K\n",
    "    K --> L\n",
    "    K --> M\n",
    "    K --> N\n",
    "    N --> O\n",
    "    L --> P\n",
    "    M --> P\n",
    "    N --> P\n",
    "    P --> Q\n",
    "    Q -->|Strong consensus| R\n",
    "    Q -->|Weak consensus| S\n",
    "    Q -->|No consensus| T\n",
    "\n",
    "    style Cleanlab fill:#e1f5fe\n",
    "    style Analysis fill:#fff3e0\n",
    "    style YRSN fill:#e8f5e9\n",
    "    style Routing fill:#fce4ec\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Collapse Type Focus**: CLASH (approvers disagree)\n",
    "\n",
    "**Difficulty**: ‚≠ê‚≠ê Medium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install cleanlab scikit-learn pandas numpy --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Cleanlab multi-annotator functions\n",
    "from cleanlab.multiannotator import (\n",
    "    get_label_quality_multiannotator,\n",
    "    get_majority_vote_label,\n",
    "    get_active_learning_scores\n",
    ")\n",
    "\n",
    "# Import YRSN adapter\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from yrsn_iars.adapters.cleanlab_adapter import CleanlabAdapter, YRSNResult\n",
    "\n",
    "print(\"Dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Generate Committee Decision Data\n",
    "\n",
    "Simulate a scenario where 5 approvers vote on each request, with varying levels of agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "n_annotators = 5  # 5 committee members\n",
    "n_classes = 3  # approve, reject, defer\n",
    "\n",
    "# Annotator characteristics (some are biased)\n",
    "annotator_biases = {\n",
    "    'approver_A': {'approve': 0.5, 'reject': 0.3, 'defer': 0.2},  # Balanced\n",
    "    'approver_B': {'approve': 0.7, 'reject': 0.2, 'defer': 0.1},  # Lenient\n",
    "    'approver_C': {'approve': 0.2, 'reject': 0.6, 'defer': 0.2},  # Strict\n",
    "    'approver_D': {'approve': 0.4, 'reject': 0.4, 'defer': 0.2},  # Balanced\n",
    "    'approver_E': {'approve': 0.3, 'reject': 0.3, 'defer': 0.4},  # Often defers\n",
    "}\n",
    "\n",
    "# Generate \"true\" labels (what the decision should be)\n",
    "true_labels = np.random.choice([0, 1, 2], n_samples, p=[0.4, 0.4, 0.2])  # 0=approve, 1=reject, 2=defer\n",
    "\n",
    "# Generate annotator labels (with noise based on biases)\n",
    "labels_multiannotator = np.full((n_samples, n_annotators), np.nan)\n",
    "\n",
    "annotator_names = list(annotator_biases.keys())\n",
    "class_names = ['approve', 'reject', 'defer']\n",
    "\n",
    "for i in range(n_samples):\n",
    "    true_label = true_labels[i]\n",
    "    \n",
    "    for j, (name, bias) in enumerate(annotator_biases.items()):\n",
    "        # Probability of agreeing with true label\n",
    "        agree_prob = 0.7  # Base agreement rate\n",
    "        \n",
    "        if np.random.random() < agree_prob:\n",
    "            # Agree with true label\n",
    "            labels_multiannotator[i, j] = true_label\n",
    "        else:\n",
    "            # Vote according to annotator's bias\n",
    "            probs = [bias['approve'], bias['reject'], bias['defer']]\n",
    "            labels_multiannotator[i, j] = np.random.choice([0, 1, 2], p=probs)\n",
    "        \n",
    "        # Some annotators miss some requests (10% NaN rate)\n",
    "        if np.random.random() < 0.1:\n",
    "            labels_multiannotator[i, j] = np.nan\n",
    "\n",
    "print(f\"Generated {n_samples} committee decisions with {n_annotators} annotators\")\n",
    "print(f\"Missing annotation rate: {np.isnan(labels_multiannotator).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create request metadata\n",
    "requests_df = pd.DataFrame({\n",
    "    'request_id': [f'REQ-{i:04d}' for i in range(n_samples)],\n",
    "    'amount': np.random.randint(1000, 100000, n_samples),\n",
    "    'category': np.random.choice(['software', 'travel', 'vendor', 'equipment'], n_samples),\n",
    "    'true_label': true_labels\n",
    "})\n",
    "\n",
    "# Add annotator votes as columns\n",
    "for j, name in enumerate(annotator_names):\n",
    "    requests_df[name] = labels_multiannotator[:, j]\n",
    "\n",
    "requests_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Cleanlab Multi-Annotator Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get majority vote labels\n",
    "majority_vote = get_majority_vote_label(labels_multiannotator)\n",
    "\n",
    "print(\"Majority Vote Distribution:\")\n",
    "print(pd.Series(majority_vote).value_counts().rename({0: 'approve', 1: 'reject', 2: 'defer'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get label quality scores for multi-annotator setting\n",
    "# Note: This requires pred_probs from a trained model\n",
    "\n",
    "# First, train a simple model on majority vote labels\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Use simple features\n",
    "X = np.column_stack([\n",
    "    requests_df['amount'].values,\n",
    "    pd.get_dummies(requests_df['category']).values\n",
    "])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train classifier\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_scaled, majority_vote)\n",
    "\n",
    "# Get predicted probabilities\n",
    "pred_probs = clf.predict_proba(X_scaled)\n",
    "\n",
    "print(f\"Classifier trained on majority vote labels\")\n",
    "print(f\"Prediction shape: {pred_probs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get multi-annotator label quality\n",
    "multiannotator_results = get_label_quality_multiannotator(\n",
    "    labels_multiannotator=labels_multiannotator,\n",
    "    pred_probs=pred_probs,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"Multi-annotator analysis results:\")\n",
    "print(f\"Keys: {multiannotator_results.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract key metrics\n",
    "consensus_label = multiannotator_results['label_quality']['consensus_label']\n",
    "consensus_quality = multiannotator_results['label_quality']['consensus_quality_score']\n",
    "\n",
    "# Annotator agreement for each example\n",
    "annotator_quality = multiannotator_results.get('annotator_stats', {}).get('quality_of_annotator', None)\n",
    "\n",
    "# Add to dataframe\n",
    "requests_df['consensus_label'] = consensus_label\n",
    "requests_df['consensus_quality'] = consensus_quality\n",
    "\n",
    "print(\"Consensus Quality Statistics:\")\n",
    "print(requests_df['consensus_quality'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 4. Compute Annotator Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_agreement_score(row_labels):\n",
    "    \"\"\"Compute agreement score for a single example's annotator labels.\"\"\"\n",
    "    valid = row_labels[~np.isnan(row_labels)]\n",
    "    if len(valid) <= 1:\n",
    "        return 1.0  # Can't measure disagreement with 1 annotator\n",
    "    \n",
    "    # Agreement = fraction of annotators who voted for the majority\n",
    "    majority = pd.Series(valid).mode()[0]\n",
    "    agreement = (valid == majority).sum() / len(valid)\n",
    "    return agreement\n",
    "\n",
    "# Compute agreement for each request\n",
    "requests_df['annotator_agreement'] = [\n",
    "    compute_agreement_score(labels_multiannotator[i])\n",
    "    for i in range(n_samples)\n",
    "]\n",
    "\n",
    "# Count annotations per example\n",
    "requests_df['n_annotations'] = (~np.isnan(labels_multiannotator)).sum(axis=1)\n",
    "\n",
    "print(\"Agreement Statistics:\")\n",
    "print(requests_df['annotator_agreement'].describe())\n",
    "print(f\"\\nHigh agreement (>=0.8): {(requests_df['annotator_agreement'] >= 0.8).sum()}\")\n",
    "print(f\"Low agreement (<0.6): {(requests_df['annotator_agreement'] < 0.6).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 5. YRSN Decomposition from Multi-Annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the YRSN adapter for multi-annotator\n",
    "adapter = CleanlabAdapter()\n",
    "\n",
    "yrsn_df = adapter.from_multiannotator(\n",
    "    labels_multiannotator=labels_multiannotator,\n",
    "    consensus_label=requests_df['consensus_label'].values,\n",
    "    annotator_agreement=requests_df['annotator_agreement'].values,\n",
    "    quality_of_consensus=requests_df['consensus_quality'].values,\n",
    "    num_annotations=requests_df['n_annotations'].values\n",
    ")\n",
    "\n",
    "# Merge with requests\n",
    "requests_df['R'] = yrsn_df['R'].values\n",
    "requests_df['S'] = yrsn_df['S'].values\n",
    "requests_df['N'] = yrsn_df['N'].values\n",
    "requests_df['collapse_type'] = yrsn_df['collapse_type'].values\n",
    "\n",
    "print(\"YRSN Statistics:\")\n",
    "print(requests_df[['R', 'S', 'N']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 6. Identify CLASH Cases (High Disagreement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find CLASH collapse cases\n",
    "clash_cases = requests_df[requests_df['collapse_type'] == 'clash'].nlargest(15, 'S')\n",
    "\n",
    "print(\"Top 15 CLASH Cases (Approver Disagreement):\")\n",
    "print(\"=\"*80)\n",
    "for _, row in clash_cases.iterrows():\n",
    "    votes = [row[name] for name in annotator_names]\n",
    "    vote_str = ', '.join([f\"{name}: {class_names[int(v)]}\" if not np.isnan(v) else f\"{name}: -\" \n",
    "                          for name, v in zip(annotator_names, votes)])\n",
    "    \n",
    "    print(f\"\\n[{row['request_id']}] Amount: ${row['amount']:,}\")\n",
    "    print(f\"R={row['R']:.2f}, S={row['S']:.2f}, N={row['N']:.2f}\")\n",
    "    print(f\"Agreement: {row['annotator_agreement']:.2f}, Consensus Quality: {row['consensus_quality']:.2f}\")\n",
    "    print(f\"Votes: {vote_str}\")\n",
    "    print(f\"Consensus: {class_names[int(row['consensus_label'])]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 7. Temperature-Based Routing for Committee Decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yrsn_iars.adapters.temperature import compute_temperature\n",
    "\n",
    "# Compute temperature\n",
    "requests_df['temperature'] = requests_df['R'].apply(lambda r: compute_temperature(r))\n",
    "\n",
    "# Routing logic for committee decisions\n",
    "def route_committee_decision(row):\n",
    "    agreement = row['annotator_agreement']\n",
    "    consensus_q = row['consensus_quality']\n",
    "    tau = row['temperature']\n",
    "    \n",
    "    # Strong consensus: auto-process\n",
    "    if agreement >= 0.8 and consensus_q >= 0.7 and tau < 1.5:\n",
    "        return 'green'\n",
    "    \n",
    "    # Moderate consensus: needs review\n",
    "    elif agreement >= 0.6 or consensus_q >= 0.5:\n",
    "        return 'yellow'\n",
    "    \n",
    "    # No consensus or clash: escalate\n",
    "    else:\n",
    "        return 'red'\n",
    "\n",
    "requests_df['stream'] = requests_df.apply(route_committee_decision, axis=1)\n",
    "\n",
    "print(\"Routing Distribution:\")\n",
    "print(requests_df['stream'].value_counts())\n",
    "print(f\"\\nAuto-approval rate: {100 * (requests_df['stream'] == 'green').mean():.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 8. Analyze Per-Annotator Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute per-annotator statistics\n",
    "annotator_stats = []\n",
    "\n",
    "for j, name in enumerate(annotator_names):\n",
    "    votes = labels_multiannotator[:, j]\n",
    "    valid_mask = ~np.isnan(votes)\n",
    "    valid_votes = votes[valid_mask]\n",
    "    valid_consensus = requests_df['consensus_label'].values[valid_mask]\n",
    "    \n",
    "    # Agreement with consensus\n",
    "    agrees_with_consensus = (valid_votes == valid_consensus).mean()\n",
    "    \n",
    "    # Vote distribution\n",
    "    vote_dist = pd.Series(valid_votes).value_counts(normalize=True)\n",
    "    \n",
    "    annotator_stats.append({\n",
    "        'annotator': name,\n",
    "        'n_votes': len(valid_votes),\n",
    "        'agrees_with_consensus': agrees_with_consensus,\n",
    "        'approve_rate': vote_dist.get(0, 0),\n",
    "        'reject_rate': vote_dist.get(1, 0),\n",
    "        'defer_rate': vote_dist.get(2, 0)\n",
    "    })\n",
    "\n",
    "annotator_df = pd.DataFrame(annotator_stats)\n",
    "\n",
    "print(\"Per-Annotator Quality:\")\n",
    "print(annotator_df.round(3))\n",
    "\n",
    "# Flag problematic annotators\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Annotator Flags:\")\n",
    "for _, row in annotator_df.iterrows():\n",
    "    flags = []\n",
    "    if row['agrees_with_consensus'] < 0.6:\n",
    "        flags.append(\"LOW CONSENSUS AGREEMENT\")\n",
    "    if row['approve_rate'] > 0.65:\n",
    "        flags.append(\"ALWAYS APPROVES\")\n",
    "    if row['reject_rate'] > 0.55:\n",
    "        flags.append(\"OFTEN REJECTS\")\n",
    "    if row['defer_rate'] > 0.35:\n",
    "        flags.append(\"OFTEN DEFERS\")\n",
    "    \n",
    "    if flags:\n",
    "        print(f\"{row['annotator']}: {', '.join(flags)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 9. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "output_cols = ['request_id', 'amount', 'category', 'consensus_label',\n",
    "               'annotator_agreement', 'consensus_quality', 'n_annotations',\n",
    "               'R', 'S', 'N', 'collapse_type', 'temperature', 'stream']\n",
    "\n",
    "requests_df[output_cols].to_csv('committee_yrsn_results.csv', index=False)\n",
    "annotator_df.to_csv('annotator_quality.csv', index=False)\n",
    "\n",
    "print(f\"Saved {len(requests_df)} committee decisions with YRSN analysis\")\n",
    "print(f\"Saved annotator quality metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we:\n",
    "1. Generated committee decision data with 5 approvers per request\n",
    "2. Used Cleanlab multi-annotator functions to find consensus\n",
    "3. Computed annotator agreement and consensus quality\n",
    "4. Mapped to YRSN: Low agreement ‚Üí high S (CLASH collapse)\n",
    "5. Applied temperature-based routing for committee decisions\n",
    "6. Analyzed per-annotator quality and flagged problematic approvers\n",
    "\n",
    "**Key Insight**: When approvers disagree, S increases (contentious case), which raises temperature and routes to yellow/red for additional review or tie-breaker.\n",
    "\n",
    "**Next**: Notebook 4 - RAG/Retrieval Context Quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
